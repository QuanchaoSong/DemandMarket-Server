{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QuanchaoSong/DemandMarket-Server/blob/master/CLIPImageModel_to_CoreML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GeS-rJ8Vhnhj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3be39544-9567-4476-bf89-101a6d613d21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install clip-benchmark>=1.4.0 datasets>=2.8.0 open-clip-torch>=2.20.0 timm>=0.9.5 coremltools\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We clone MobileCLIP from: https://github.com/apple/ml-mobileclip\n",
        "!git clone https://github.com/apple/ml-mobileclip.git\n",
        "\n",
        "#Install MobileCLIP\n",
        "%cd ml-mobileclip\n",
        "!pip install -e . -q"
      ],
      "metadata": {
        "id": "edfay2I4h9bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download pretrained checkpoints\n",
        "%mkdir -p checkpoints\n",
        "!wget wget https://docs-assets.developer.apple.com/ml-research/datasets/mobileclip/mobileclip_s0.pt -P checkpoints"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WsMmZj5kQ3g",
        "outputId": "aab5bf12-f981-4331-fea2-f1cd3dae58f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-09 13:57:51--  http://wget/\n",
            "Resolving wget (wget)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘wget’\n",
            "--2024-07-09 13:57:51--  https://docs-assets.developer.apple.com/ml-research/datasets/mobileclip/mobileclip_s0.pt\n",
            "Resolving docs-assets.developer.apple.com (docs-assets.developer.apple.com)... 17.253.7.145, 17.253.7.134, 2620:149:a16:f000::140, ...\n",
            "Connecting to docs-assets.developer.apple.com (docs-assets.developer.apple.com)|17.253.7.145|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 215934653 (206M) [application/octet-stream]\n",
            "Saving to: ‘checkpoints/mobileclip_s0.pt’\n",
            "\n",
            "mobileclip_s0.pt    100%[===================>] 205.93M   155MB/s    in 1.3s    \n",
            "\n",
            "2024-07-09 13:57:53 (155 MB/s) - ‘checkpoints/mobileclip_s0.pt’ saved [215934653/215934653]\n",
            "\n",
            "FINISHED --2024-07-09 13:57:53--\n",
            "Total wall clock time: 1.7s\n",
            "Downloaded: 1 files, 206M in 1.3s (155 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model inference with reparamerized model\n",
        "import coremltools\n",
        "import torch\n",
        "import mobileclip\n",
        "from mobileclip.modules.common.mobileone import reparameterize_model\n",
        "from mobileclip.modules.text.tokenizer import (\n",
        "    ClipTokenizer,\n",
        ")\n",
        "from mobileclip.clip import CLIP\n",
        "from typing import Dict, Optional, Any\n",
        "import json\n",
        "import os\n",
        "\n",
        "# mobileclip_s0\n",
        "model_cfg = {\n",
        "    \"embed_dim\": 512,\n",
        "    \"image_cfg\": {\n",
        "        \"image_size\": 256,\n",
        "        \"model_name\": \"mci0\"\n",
        "    },\n",
        "    \"text_cfg\": {\n",
        "        \"context_length\": 77,\n",
        "        \"vocab_size\": 49408,\n",
        "        \"dim\": 512,\n",
        "        \"ffn_multiplier_per_layer\": 4.0,\n",
        "        \"n_heads_per_layer\": 8,\n",
        "        \"n_transformer_layers\": 4,\n",
        "        \"norm_layer\": \"layer_norm_fp32\",\n",
        "        \"causal_masking\": False,\n",
        "        \"model_name\": \"mct\"\n",
        "    }\n",
        "}\n",
        "\n",
        "class CLIP_encode_image(CLIP):\n",
        "    \"\"\"Class for encoding images using the image encoder from CLIP.\"\"\"\n",
        "\n",
        "    def __init__(self, cfg: Dict, output_dict: bool = False, *args, **kwargs) -> None:\n",
        "        super().__init__(cfg, output_dict, *args, **kwargs)\n",
        "\n",
        "    def forward(self, image: Optional[torch.Tensor] = None) -> Any:\n",
        "        image_embeddings = (\n",
        "            self.encode_image(image, normalize=True) if image is not None else None\n",
        "        )\n",
        "        return image_embeddings\n",
        "\n",
        "\n",
        "model_ie = CLIP_encode_image(cfg=model_cfg)\n",
        "model_ie.eval()\n",
        "\n",
        "chkpt = torch.load(\"checkpoints/mobileclip_s0.pt\")\n",
        "model_ie.load_state_dict(chkpt)\n",
        "\n",
        "reparameterized_model = reparameterize_model(model_ie)\n",
        "reparameterized_model.eval()\n",
        "\n",
        "\n",
        "image = torch.rand(1, 3, 256, 256)\n",
        "traced_model = torch.jit.trace(reparameterized_model, image)\n",
        "\n",
        "# Define the input as an image type\n",
        "input_image = coremltools.ImageType(name=\"input_image\", shape=(1, 3, 256, 256), color_layout=coremltools.colorlayout.RGB, scale=1/255.0, bias=[0, 0, 0])\n",
        "output_tensor = [coremltools.TensorType(name=\"output_embeddings\")]\n",
        "\n",
        "ml_model = coremltools.convert(\n",
        "        model=traced_model,\n",
        "        outputs=output_tensor,\n",
        "        inputs=[input_image],\n",
        "        convert_to=\"mlprogram\",\n",
        "        minimum_deployment_target=coremltools.target.iOS17,\n",
        "        compute_units=coremltools.ComputeUnit.ALL,\n",
        "        debug=True,\n",
        "    )\n",
        "ml_model.save(\"clip_mci_image_s0.mlpackage\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAE4VKBAjqQw",
        "outputId": "6452c789-a192-41a7-bd0a-727e2cc054b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:coremltools:scikit-learn version 1.2.2 is not supported. Minimum required version: 0.17. Maximum required version: 1.1.2. Disabling scikit-learn conversion API.\n",
            "WARNING:coremltools:XGBoost version 2.0.3 has not been tested with coremltools. You may run into unexpected errors. XGBoost 1.4.2 is the most recent version that has been tested.\n",
            "WARNING:coremltools:TensorFlow version 2.15.0 has not been tested with coremltools. You may run into unexpected errors. TensorFlow 2.12.0 is the most recent version that has been tested.\n",
            "WARNING:coremltools:Failed to load _MLModelProxy: No module named 'coremltools.libcoremlpython'\n",
            "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 730/731 [00:00<00:00, 1002.87 ops/s]\n",
            "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 17.49 passes/s]\n",
            "Running MIL default pipeline:  10%|█         | 8/78 [00:00<00:04, 16.43 passes/s]/usr/local/lib/python3.10/dist-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:266: UserWarning: Output, '1260', of the source model, has been renamed to 'var_1260' in the Core ML model.\n",
            "  warnings.warn(msg.format(var.name, new_name))\n",
            "Running MIL default pipeline: 100%|██████████| 78/78 [00:04<00:00, 17.86 passes/s]\n",
            "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 35.73 passes/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check CoreML Parameters:\n",
        "spec = ml_model.get_spec()\n",
        "print(\"model type: {}\".format(spec.WhichOneof('Type')))\n",
        "print(\"model description: {}\".format(spec.description))\n",
        "print(\"model inputs: {}\".format(spec.description.input))\n",
        "print(\"model outputs: {}\".format(spec.description.output))"
      ],
      "metadata": {
        "id": "tkG9QhmSsWfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download mlpackage\n",
        "import shutil\n",
        "from google.colab import files\n",
        "directory_path = 'clip_mci_image_s0.mlpackage'\n",
        "zip_path = 'clip_mci_image_s0.mlpackage.zip'\n",
        "shutil.make_archive(directory_path, 'zip', directory_path)\n",
        "os.rename(f\"{directory_path}.zip\", zip_path)\n",
        "files.download(zip_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "MdWzx9HEmhBR",
        "outputId": "1d217c06-5be9-4809-c5d4-7da58be9dbb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_79558fc5-ebb8-41dd-afe4-cef35c20ef88\", \"clip_mci_image_s0.mlpackage.zip\", 21155901)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Optional: Visualize the model\n",
        "!pip install graphviz torchview\n",
        "from torchview import draw_graph\n",
        "import graphviz\n",
        "graphviz.set_jupyter_format('png')\n",
        "model_graph = draw_graph(reparameterized_model, input_data = image, expand_nested = True, depth = 5)\n",
        "model_graph.visual_graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "OiVjROoqo_Za",
        "outputId": "63f81492-1ae7-4264-842e-ab2f405b690d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (0.20.3)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchview'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d02190300114>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Optional: Visualize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install graphviz torchview'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchview\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraw_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_jupyter_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchview'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5_gERzgpg41f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}